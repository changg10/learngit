```
import gradio as gr
from transformers import AutoProcessor
from vllm import LLM, SamplingParams
from qwen_vl_utils import process_vision_info

# --- 1. 配置模型路径 ---
# 请将此路径修改为您在本地服务器上下载的模型权重文件夹路径
MODEL_PATH = "/data/Cosmos-Reason1-7B" 

# --- 2. 初始化vLLM引擎 ---
# tensor_parallel_size 可根据您的GPU数量进行调整
llm = LLM(
    model=MODEL_PATH,
    tensor_parallel_size=4, 
    pipeline_parallel_size=1,
    limit_mm_per_prompt={"image": 10, "video": 10},
)

# --- 3. 设置采样参数 ---
sampling_params = SamplingParams(
    temperature=0.6,
    top_p=0.95,
    repetition_penalty=1.05,
    max_tokens=4096,
)

# --- 4. 加载处理器 ---
processor = AutoProcessor.from_pretrained(MODEL_PATH)

# --- 5. 定义核心处理函数 ---
def parse_model_output(generated_text):
    """解析模型的输出，分离思考过程和最终答案。"""
    think, answer = "", ""
    # 分离<think>标签
    if "</think>" in generated_text:
        think_split = generated_text.split("</think>")
        think = think_split[0].replace("<think>", "").strip()
        answer_part = "</think>".join(think_split[1:]).strip()
    else:
        answer_part = generated_text
    
    # 分离<answer>标签
    if "<answer>" in answer_part and "</answer>" in answer_part:
        answer = answer_part.split("<answer>")[1].split("</answer>")[0].strip()
    else:
        answer = answer_part.strip()
        
    return think, answer

def video_chat(video_path, user_prompt):
    """处理视频和文本输入，并返回模型的推理结果。"""
    if not video_path or not user_prompt:
        return "请输入视频和问题！", "请输入视频和问题！"

    messages = [
        {"role": "system", "content": "You are a helpful assistant. Answer the question in the following format: <think>your thought process</think>\n\n<answer>\nyour answer\n</answer>."},
        {
            "role": "user",
            "content": [
                {"type": "text", "text": user_prompt},
                {"type": "video", "video": video_path, "fps": 4}
            ]
        },
    ]
    
    # 构建Prompt
    prompt = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    
    # 处理视觉信息
    image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)
    mm_data = {}
    if image_inputs is not None: mm_data["image"] = image_inputs
    if video_inputs is not None: mm_data["video"] = video_inputs

    llm_inputs = {
        "prompt": prompt,
        "multi_modal_data": mm_data,
        "mm_processor_kwargs": video_kwargs,
    }
    
    # 生成结果
    outputs = llm.generate([llm_inputs], sampling_params=sampling_params)
    generated_text = outputs[0].outputs[0].text
    
    # 解析并返回结果
    think, answer = parse_model_output(generated_text)
    return think, answer

# --- 6. 构建Gradio Web界面 ---
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# 🤖 Cosmos-Reason1-7B 视频理解Demo")
    gr.Markdown("上传一段视频，提出你的问题，看看AI如何“思考”和回答。")
    
    with gr.Row():
        with gr.Column(scale=1):
            video_input = gr.Video(label="上传视频 (MP4)")
            prompt_input = gr.Textbox(label="请输入你的问题", lines=2, placeholder="例如：视频里的人正在干什么？他接下来最可能做什么？")
            submit_btn = gr.Button("🚀 提交", variant="primary")
            
        with gr.Column(scale=1):
            with gr.Accordion("🧠 AI的思考过程（点击展开/收起）", open=False):
                think_output = gr.Textbox(label="Thinking Process", lines=10, interactive=False)
            answer_output = gr.Textbox(label="✅ 最终答案", lines=6, interactive=False)

    submit_btn.click(
        video_chat,
        inputs=[video_input, prompt_input],
        outputs=[think_output, answer_output]
    )

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
```

