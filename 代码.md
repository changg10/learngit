```
import gradio as gr
from transformers import AutoProcessor
from vllm import LLM, SamplingParams
from qwen_vl_utils import process_vision_info

# --- 1. é…ç½®æ¨¡å‹è·¯å¾„ ---
# è¯·å°†æ­¤è·¯å¾„ä¿®æ”¹ä¸ºæ‚¨åœ¨æœ¬åœ°æœåŠ¡å™¨ä¸Šä¸‹è½½çš„æ¨¡å‹æƒé‡æ–‡ä»¶å¤¹è·¯å¾„
MODEL_PATH = "/data/Cosmos-Reason1-7B" 

# --- 2. åˆå§‹åŒ–vLLMå¼•æ“ ---
# tensor_parallel_size å¯æ ¹æ®æ‚¨çš„GPUæ•°é‡è¿›è¡Œè°ƒæ•´
llm = LLM(
    model=MODEL_PATH,
    tensor_parallel_size=4, 
    pipeline_parallel_size=1,
    limit_mm_per_prompt={"image": 10, "video": 10},
)

# --- 3. è®¾ç½®é‡‡æ ·å‚æ•° ---
sampling_params = SamplingParams(
    temperature=0.6,
    top_p=0.95,
    repetition_penalty=1.05,
    max_tokens=4096,
)

# --- 4. åŠ è½½å¤„ç†å™¨ ---
processor = AutoProcessor.from_pretrained(MODEL_PATH)

# --- 5. å®šä¹‰æ ¸å¿ƒå¤„ç†å‡½æ•° ---
def parse_model_output(generated_text):
    """è§£ææ¨¡å‹çš„è¾“å‡ºï¼Œåˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆã€‚"""
    think, answer = "", ""
    # åˆ†ç¦»<think>æ ‡ç­¾
    if "</think>" in generated_text:
        think_split = generated_text.split("</think>")
        think = think_split[0].replace("<think>", "").strip()
        answer_part = "</think>".join(think_split[1:]).strip()
    else:
        answer_part = generated_text
    
    # åˆ†ç¦»<answer>æ ‡ç­¾
    if "<answer>" in answer_part and "</answer>" in answer_part:
        answer = answer_part.split("<answer>")[1].split("</answer>")[0].strip()
    else:
        answer = answer_part.strip()
        
    return think, answer

def video_chat(video_path, user_prompt):
    """å¤„ç†è§†é¢‘å’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶è¿”å›æ¨¡å‹çš„æ¨ç†ç»“æœã€‚"""
    if not video_path or not user_prompt:
        return "è¯·è¾“å…¥è§†é¢‘å’Œé—®é¢˜ï¼", "è¯·è¾“å…¥è§†é¢‘å’Œé—®é¢˜ï¼"

    messages = [
        {"role": "system", "content": "You are a helpful assistant. Answer the question in the following format: <think>your thought process</think>\n\n<answer>\nyour answer\n</answer>."},
        {
            "role": "user",
            "content": [
                {"type": "text", "text": user_prompt},
                {"type": "video", "video": video_path, "fps": 4}
            ]
        },
    ]
    
    # æ„å»ºPrompt
    prompt = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    
    # å¤„ç†è§†è§‰ä¿¡æ¯
    image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)
    mm_data = {}
    if image_inputs is not None: mm_data["image"] = image_inputs
    if video_inputs is not None: mm_data["video"] = video_inputs

    llm_inputs = {
        "prompt": prompt,
        "multi_modal_data": mm_data,
        "mm_processor_kwargs": video_kwargs,
    }
    
    # ç”Ÿæˆç»“æœ
    outputs = llm.generate([llm_inputs], sampling_params=sampling_params)
    generated_text = outputs[0].outputs[0].text
    
    # è§£æå¹¶è¿”å›ç»“æœ
    think, answer = parse_model_output(generated_text)
    return think, answer

# --- 6. æ„å»ºGradio Webç•Œé¢ ---
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¤– Cosmos-Reason1-7B è§†é¢‘ç†è§£Demo")
    gr.Markdown("ä¸Šä¼ ä¸€æ®µè§†é¢‘ï¼Œæå‡ºä½ çš„é—®é¢˜ï¼Œçœ‹çœ‹AIå¦‚ä½•â€œæ€è€ƒâ€å’Œå›ç­”ã€‚")
    
    with gr.Row():
        with gr.Column(scale=1):
            video_input = gr.Video(label="ä¸Šä¼ è§†é¢‘ (MP4)")
            prompt_input = gr.Textbox(label="è¯·è¾“å…¥ä½ çš„é—®é¢˜", lines=2, placeholder="ä¾‹å¦‚ï¼šè§†é¢‘é‡Œçš„äººæ­£åœ¨å¹²ä»€ä¹ˆï¼Ÿä»–æ¥ä¸‹æ¥æœ€å¯èƒ½åšä»€ä¹ˆï¼Ÿ")
            submit_btn = gr.Button("ğŸš€ æäº¤", variant="primary")
            
        with gr.Column(scale=1):
            with gr.Accordion("ğŸ§  AIçš„æ€è€ƒè¿‡ç¨‹ï¼ˆç‚¹å‡»å±•å¼€/æ”¶èµ·ï¼‰", open=False):
                think_output = gr.Textbox(label="Thinking Process", lines=10, interactive=False)
            answer_output = gr.Textbox(label="âœ… æœ€ç»ˆç­”æ¡ˆ", lines=6, interactive=False)

    submit_btn.click(
        video_chat,
        inputs=[video_input, prompt_input],
        outputs=[think_output, answer_output]
    )

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
```

